[{"content":"Hybrid Encryption: Symmetric and Asymmetric Encryption Combined Both symmetric and asymmetric encryption has advantages and disadvantages. So, which one should we use? Well, nowadays we often use them together. Asymmetric encryption is often used to exchange private keys between parties securely. In other words, parties who would communicate establish an asymmetric encryption protocol in the beginning just to exchange private keys. When the private key exchange is completed, they keep communicating by using symmetric encryption - which is faster than asymmetric encryption. This is also how SSL/TLS works.\nUsing symmetric and asymmetric encryption together is also known as Hybrid Encryption.\nHybrid Encryption for Communications: Messaging Here is a sample scenario for hybrid communications:\n Bob wants to send an e-mail to Alice. Bob gets Alice\u0026rsquo;s digital certificate, which includes her name and her public key.  Alice\u0026rsquo;s certificate must be signed by a CA (certificate authority). In other words, it must be encrypted with a CA\u0026rsquo;s private key.   Bob decrypts Alice\u0026rsquo;s certificate, by using CA\u0026rsquo;s public key, and reveals her public key. Bob generates a symmetric secret key pair (two identical keys). Bob uses Alice\u0026rsquo;s public key to encrypt one of these symmetric keys. The only key that can decrypt this message and reveal the symmetric key is Alice\u0026rsquo;s private key. Therefore, Bob is safe to send this key over the wire, and he sends it to Alice. Alice then uses her private key to decrypt the message and to reveal the symmetric key of Bob. From now on, they both can use this symmetric key to encrypt and decrypt messages between them. Briefly, asymmetric encryption is used to facilitate a key exchange.   This is exactly how SSL/TLS works!\n SSL/TLS Handshake  The content under this title is identical with the steps explained under \u0026ldquo;Hybrid Encryption for Communications: Messaging\u0026rdquo; Alice here replaced by google.com\n  Bob wants to connect to google.com. Bob gets google.com\u0026rsquo;s digital certificate, which includes the organization name and their public key.  google.com\u0026rsquo;s certificate must be signed by a CA (certificate authority). In other words, it must be encrypted with a CA\u0026rsquo;s private key.   Bob decrypts google.com\u0026rsquo;s certificate, by using CA\u0026rsquo;s public key, and reveals their public key. Bob generates a symmetric secret key pair (two identical keys). Bob uses google.com\u0026rsquo;s public key to encrypt one of these symmetric keys. The only key that can decrypt this message and reveal the symmetric key is google.com\u0026rsquo;s private key. Therefore, Bob is safe to send this key over the wire, and he sends it to google.com. google.com then uses their private key to decrypt the message and to reveal the symmetric key of Bob. From now on, they both can use this symmetric key to encrypt and decrypt messages between them. Briefly, asymmetric encryption is used to facilitate a key exchange.  SSL or TLS? Often SSL and TLS are used interchangeably. However, they aren\u0026rsquo;t identical terms. SSL (Secure Sockets Layer) is a deprecated cryptographic protocol and totally by TLS (Transport Layer Security). The last time SSL was updated was in 1996.\nSSL and TLS have three main purposes:\n Confidentiality (Encryption): Data is only accessible by Client and Server. Integrity (Hashing): Data isn\u0026rsquo;t modified between Client and Server. Authentication (PKI): Client/Server is indeed who they say they are.  TLS Downgrade Attack TLS is prone to \u0026ldquo;Downgrade Attack\u0026rdquo;. In this attack type, the client (web browser) fakes the highest supported TLS version with an old version. For example, the client requests TLS 1.0, even though it can support TLS 1.3. By doing so, the attacker tries to be served by a weaker TLS version and manipulate it. To prevent your systems from this attack, you should configure your server not to support lower TLS versions.\nWho are these CAs (Certificate Authorities) anyway?  Today, 5 organizations (SSL certificate issuers) secure 98% of the Internet:  IdenTrust (51.9%)  Let\u0026rsquo;s Encrypt   DigiCert (19.4%)  GeoTrust Verisign Thawte   Sectigo (17.5%)  Comodo   GoDaddy (6.9%) GlobalSign (2.9%)    However, they don\u0026rsquo;t issue all certificates themselves. Often we obtain our certificates from intermediate CAs, which is a topic of PKI (Public Key Infrastructure).\nPublic Key Infrastructure (PKI) CA\u0026rsquo;s use self-signed certificates! But why? Because we need to trust someone, at the top! At the top, there is a CA, which has a self-signed certificate, such as IdenTrust. Then there are intermediate CAs, under the root CA, but their certificates aren\u0026rsquo;t self-signed. Instead, they are signed by a CA (with CA\u0026rsquo;s private key). These intermediate CAs can issue (sign) certificates for us. This is known as the \u0026ldquo;Chain of Trust\u0026rdquo;. We trust our certificate provider, our certificate provider trusts the root CA, and so on. By the way, it\u0026rsquo;s also possible to set up your own corporate CA if you don\u0026rsquo;t want to pay for certificates every single time.\nIn other to verify the Chain of Trust, we use their public keys. Let\u0026rsquo;s say google.com has a certificate issued by CA1. To verify this, we check the certificate of CA1. The thing is, the certificate of CA1 is issued by RootCA. Now we need to verify everything one by one. Our browser will verify the RootCA by checking the local trust store. Then the certificate of CA1 by using the public key of RootCA. Then the certificate of google.com, by using the public key of CA1.\nThree entities form a PKI:\n Client: Needs to connect securely or verify identity. Server: Needs to prove its identity. Certificate Authority: Validate identities \u0026amp; generate certificates.  ","permalink":"https://www.serhatdundar.com/posts/ssl-tls-handshake-hybrid-encryption/","summary":"Hybrid Encryption: Symmetric and Asymmetric Encryption Combined Both symmetric and asymmetric encryption has advantages and disadvantages. So, which one should we use? Well, nowadays we often use them together. Asymmetric encryption is often used to exchange private keys between parties securely. In other words, parties who would communicate establish an asymmetric encryption protocol in the beginning just to exchange private keys. When the private key exchange is completed, they keep communicating by using symmetric encryption - which is faster than asymmetric encryption.","title":"SSL/TLS Handshake, Hybrid Encryption and Public Key Infrastructure (PKI)"},{"content":"Symmetric Encryption (Private Key Cryptography) In symmetric encryption only a single key, in other words, a private key is used to encrypt and decrypt a message. Symmetric encryption is also known as \u0026ldquo;Private Key Cryptography\u0026rdquo; as the whole encryption is only based on a private key.\nSome popular symetric encryption algorithms are:\n   Algorithm Cipher Key Size Block/State Size Popularity Notes     AES Block 128, 192, or 256 bits 128 bits 1 The best one. Still in use.   Blowfish Block 32-448 bits 64 bits 2 Still quite safe, but slower than AES.   Twofish Block 128, 192, or 256 bits 128 bits 3 Still quite safe, but slower than AES.   DES Block 56 bits 64 bits 4 Not considered as safe anymore.   3DES Block 112, or 168 bits 64 bits 4 Not considered as safe anymore.   RC4 Stream 40-2048 bits 2064 bits 4 Not considered as safe anymore.    Pros of symmetric encryption:\n It\u0026rsquo;s fast. Much faster than asymmetric encryption. Ciphertext is the same size as plain text. There is no cipher-text expansion.  Cons of symmetric encryption:\n Securely exchanging the private key on any medium is the biggest challenge as the key can be stolen during the exchange. Every person with a copy of a private key is now a security risk because they can make the key stolen. Rotating the private key is challenging as all copies of the private key also need to be rotated.  Block and Stream Ciphers Any symmetric encryption algorithm either uses a block or a stream cipher.\n   Block Ciphers Stream Ciphers     Used to encrypt chunks or blocks of data Used to encrypt streams of data   Good for known sizes of data Better for unknown sizes of data   More popular than stream ciphers Not commonly used these days   Examples: AES, Blowfish, Twofish, DES, 3DES Examples: RC4    Stream ciphers are less secure than block ciphers as a malicious hacker can flip a bit while the algorithm is running an XOR bit-by-bit. Flipping a single bit generates a whole different message and wreck the output in block ciphers as they are encrypted altogether as a block.\nHowever, stream ciphers can be protected with HMAC (Hashed Message Authentication Code).\nAsymmetric Encryption (Public Key Cryptography)  Asymmetric encryption is also known as Public Key Cryptograhy. In asymmetric encryption, two different keys (public key and private key) are used for encryption and decryption. As the name suggests, the public key can be distributed publicly over the wire. However, the private key must be protected. These two keys (public key and private key) are mathematically related pairs, but still, they aren\u0026rsquo;t identical copies. In asymmetric encryption, the public key is used to encrypt a message, and a private key is used to decrypt a message. However, asymmetric encryption algorithms can also be used for signing purposes. In this case, the private key is used to sign a message, and a public key is used to verify a signature. Keep in mind, encryption and signing serve different purposes! Encryption is all about confidentiality, and signing is all about integrity and authentication. Asymmetric encryption algorithms in general, depends on huge prime number calculations that are impossible to break if the key size is sufficient.  Some popular asymetric encryption algorithms are:\n   Algorithm Purpose/Usage     RSA Encryption, signatures, key exchange   DSA Signatures   Diffie-Hellman Key exchange   ElGamal Encryption, signatures    Pros of asymmetric encryption:\n No key exchange concerns. The public key can be shared publicly over the wire.  Cons of asymmetric encryption:\n Compared to symmetric encryption, it\u0026rsquo;s very slow and not efficient for large data. Cipher Text Expansion: The ciphertext is larger than the original message.  Asymmetric Encryption and Man in the Middle In theory, the public key can be shared publicly over the Internet, and anyone can use this key to encrypt their messages and send them back to us. However, how can we be sure of the public key that we are using for encryption? Since anyone can create a key pair and publish it on the Internet, this model is open to impersonation. What if an attacker tricks you with their public key? Since you will be encrypting your message with the attacker\u0026rsquo;s public key, they will be able to read all messages with their private key. They can even do that without being noticed. This is how it works:\n An attacker replaces a legit public key with their public key. The victim encrypts their message with the attacker\u0026rsquo;s public key and sends the encrypted message. The attacker receives the message, decrypts it with their private key, and then encrypts it with the legit key again and sends the encrypted message to the legit key owner. By doing so, the attacker can remain undetected between the communication.  So, how do we deal with this Man in the Middle attack? The short answer is signing and certificates.\nDigital Signatures Digital signatures are all about Integrity, Authentication, and Non-Repudiation. However, they don\u0026rsquo;t provide confidentiality. Therefore you can think of hashing algorithms as they are the default way of proving integrity when you hear about digital signatures.\nAnything can be signed digitally. An e-mail, a file, a message - anything you can think of.\nWhatever you\u0026rsquo;re signing should go through a hashing algorithm to generate a digest. When a digest is encrypted with the sender\u0026rsquo;s private key it\u0026rsquo;s called a digital signature. The receiver uses the sender\u0026rsquo;s public key to decrypt the message and to reveal the digest. Then the receiver generates the hash again and compares it with what they got after decryption.\nDigital Certificates One of the problems with asymmetric encryption is, we can never be sure if we are using the real public key of the recipient. Therefore, an independent and trusted authority, similar to a notary, that binds a public key with a name is required. When a name with a public key is bound, it\u0026rsquo;s called a digital certificate.\n A digital certificate includes the following:  Owner\u0026rsquo;s name Owner\u0026rsquo;s public key and its expiration date The certificate issuer\u0026rsquo;s name The certificate issuer\u0026rsquo;s digital signature    A digital certificate is based on trust (chain of trust), to the issuer. When using digital certificates, the trusted the third party is a certificate authority, an entity that issues digital certificates.\nSample Scenarios Asymmetric Encryption for Confidentiality: Without Digital Certificates  Receiver\u0026rsquo;s public key is used for encryption This is how GPG/PGP encryption process works.\n  Bob wants to send an e-mail to Alice. Bob uses Alice\u0026rsquo;s public key to encrypt his message. Alice receives the e-mail and decrypts it with her private key.      Guaranteed? Why?     Confidentiality Yes The message is encrypted. Only Alice, with her private key, can decrypt it   Integrity Yes Alice can decrypt the message. That proves the message is encrypted with her public key   Non-repudiation/Authentication No Anyone can get Alice\u0026rsquo;s public key, encrypt a message with it, and send it to her    Man-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can\u0026rsquo;t decrypt the message as they will need the private key of Alice. The attacker can\u0026rsquo;t also modify the message because if they do, Alice won\u0026rsquo;t be able to decrypt it with her private message.  Risks:\n If an attacker can replace Alice\u0026rsquo;s public key, or somehow trick Bob with their public key, then they can control the whole communication without being noticed. Alice can never be sure of the sender, as anyone can send her a message by using her certification.  Asymmetric Encryption for Confidentiality: With Digital Certificates  Receiver\u0026rsquo;s public key is used for encryption\n  Bob wants to send an e-mail to Alice. Bob fetches Alice\u0026rsquo;s certificate, obtains her public key, and checks the \u0026ldquo;certificate owner\u0026rsquo;s name\u0026rdquo; to make sure it belongs to Alice. Bob uses Alice\u0026rsquo;s public key to encrypt his message. Alice receives the e-mail and decrypts it with her private key.      Guaranteed? Why?     Confidentiality Yes The message is encrypted. Only Alice, with her private key, can decrypt it   Integrity Yes Alice can decrypt the message. That proves the message is encrypted with her public key   Non-repudiation/Authentication No Anyone can get Alice\u0026rsquo;s certificate, encrypt a message with it, and send it to her    Man-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can\u0026rsquo;t decrypt the message as they will need the private key of Alice. The attacker can\u0026rsquo;t also modify the message because if they do, Alice won\u0026rsquo;t be able to decrypt it with her private message.  Risks:\n Alice can never be sure of the sender, as anyone can send her a message by using her certification.  Asymmetric Encryption for Integrity and Authentication: Without Digital Signatures  Sender\u0026rsquo;s private key is used for encryption\n  Bob wants to send an e-mail to Alice. Bob uses his private key to encrypt the message. Alice receives the e-mail and decrypts it with Bob\u0026rsquo;s public key.      Guaranteed? Why?     Confidentiality No Anyone can decrypt the encrypted message since Bob\u0026rsquo;s public key is publicly available   Integrity Yes Alice can decrypt the message. That proves, the message is encrypted with Bob\u0026rsquo;s private key and hasn\u0026rsquo;t been changed   Non-repudiation/Authentication Yes Since the message can be decrypted with Bob\u0026rsquo;s public key, it must have been encrypted with his private key    Man-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can decrypt the message as the public key of Bob is available to everyone. The attacker can\u0026rsquo;t modify the message because if they do, Bob\u0026rsquo;s public key won\u0026rsquo;t work during decryption.  The problem with this approach is, the asymmetric encryption is slow and it doesn\u0026rsquo;t work well with big messages because of the cipher text expansion (the ciphertext is larger than the original message). Therefore, we often use digital signatures for integrity and authentication purposes.\nAsymmetric Encryption for Integrity and Authentication: With Digital Signatures  Sender\u0026rsquo;s private key is used for encryption This is how GPG/PGP signing process works.\n  Bob wants to send an e-mail to Alice. Bob hashes the plain-text message and generates a digest.  The generated digest is smaller than the original message, so it saves us from the previously explained cipher text expansion issue.   Bob uses his private key to encrypt the digest.  This encrypted digest is called as digital signature.   Bob sends the message in plaintext and the digital signature to Alice. Alice receives the e-mail and decrypts the digital signature with Bob\u0026rsquo;s public key to reveal the digest. Alice hashes the plaintext message and generates her digest. Then she compares the digest she created, and she got from Bob. If they match, integrity and non-repudiation will be proven.      Guaranteed? Why?     Confidentiality No The message is in plain text, and digital signatures aren\u0026rsquo;t about confidentiality   Integrity Yes Alice can decrypt the message and compare hashes. That proves, the message is encrypted with Bob\u0026rsquo;s private key and hasn\u0026rsquo;t been changed   Non-repudiation/Authentication Yes Since the message can be decrypted with Bob\u0026rsquo;s public key, it must have been encrypted with his private key    Man-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can decrypt the message and reveal the digest as the public key of Bob is available to everyone. The attacker can\u0026rsquo;t modify the message because if they do, Bob\u0026rsquo;s public key won\u0026rsquo;t work during decryption.  Asymmetric Encryption for Integrity and Authentication: With Digital Certificates  Sender\u0026rsquo;s private key is used for encryption\n  Bob wants to send an e-mail to Alice. Bob obtains a certificate by using his public key. Bob uses his private key to encrypt the message. Alice uses Bob\u0026rsquo;s certificate to obtain his public key and to verify his name. Alice decrypts the e-mail with Bob\u0026rsquo;s public key.      Guaranteed? Why?     Confidentiality No Anyone can decrypt the encrypted message since Bob\u0026rsquo;s certificate is publicly available   Integrity Yes Alice can decrypt the message. That proves, the message is encrypted with Bob\u0026rsquo;s private key and hasn\u0026rsquo;t been changed   Non-repudiation/Authentication Yes Since the message can be decrypted with Bob\u0026rsquo;s certificate, it must have been encrypted with his private key    Man-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can decrypt the message as the certificate of Bob is available to everyone. The attacker can\u0026rsquo;t modify the message because if they do, Bob\u0026rsquo;s certificate won\u0026rsquo;t work during decryption.  Risks:\n No confidentiality as Bob\u0026rsquo;s certificate is available to everyone.  Asymmetric Encryption for Confidentiality, Integrity and Authentication: With Digital Certificates  Sender\u0026rsquo;s private key is used for encryption\n  Bob wants to send an e-mail to Alice. Bob fetches Alice\u0026rsquo;s certificate, obtains her public key, and checks the \u0026ldquo;certificate owner\u0026rsquo;s name\u0026rdquo; to make sure it belongs to Alice. Bob encrypts his message with his private key. Bob re-encrypts the encrypted message with Alice\u0026rsquo;s public key. Bob send the message to Alice. Alice gets the message and decrypts it with her private key to reveal the message that Bob encrypted with his private key. Alice then decrypts it with Bob\u0026rsquo;s public key.      Guaranteed? Why?     Confidentiality Yes The message is encrypted with Alice\u0026rsquo;s public key. Only Alice, with her private key, can decrypt it   Integrity Yes Alice can decrypt the message and compare hashes. That proves, the message is encrypted with Bob\u0026rsquo;s private key and hasn\u0026rsquo;t been changed   Non-repudiation/Authentication Yes Since the message can be decrypted with Bob\u0026rsquo;s public key, it must have been encrypted with his private key    The problem with this approach is, the asymmetric encryption is slow and it doesn\u0026rsquo;t work well with big messages because of the cipher text expansion (the ciphertext is larger than the original message). Since Bob uses asymmetric encryption for encrypting his message, the process will be slow and the output might be huge. Therefore, we often use digital signatures for integrity and authentication purposes.\nMan-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can\u0026rsquo;t decrypt the message as they can\u0026rsquo;t obtain Alice\u0026rsquo;s private key. The attacker can\u0026rsquo;t modify the message because if they do, Bob\u0026rsquo;s public key won\u0026rsquo;t work during decryption. The sender is guaranteed to be Bob as he signed the message with his private key, a key that attackers can\u0026rsquo;t obtain.  Asymmetric Encryption for Confidentiality, Integrity and Authentication: With Digital Certificates and Signatures  Sender\u0026rsquo;s private key is used for integrity Reciever\u0026rsquo;s public key is used for encryption\n  Bob wants to send an e-mail to Alice. Bob fetches Alice\u0026rsquo;s certificate, obtains her public key, and checks the \u0026ldquo;certificate owner\u0026rsquo;s name\u0026rdquo; to make sure it belongs to Alice. Bob hashes the plain-text message and generates a digest.  The generated digest is smaller than the original message, so it saves us from the previously explained cipher text expansion issue.   Bob uses his private key to encrypt the digest.  This encrypted digest is called as digital signature.   Bob encrypts his message with Alice\u0026rsquo;s public key. Bob sends the encrypted message and the digital signature to Alice. Alice receives the e-mail and  Decrypts the digital signature with Bob\u0026rsquo;s public key to reveal the digest. Decrypts the encrypted message with her private key.   Alice hashes the decrypted message and generates her digest. Then she compares the digest she created, and she got from Bob. If they match, integrity and non-repudiation will be proven.      Guaranteed? Why?     Confidentiality Yes The message is encrypted with Alice\u0026rsquo;s public key. Only Alice, with her private key, can decrypt it   Integrity Yes Alice can decrypt the message and compare hashes. That proves, the message is encrypted with Bob\u0026rsquo;s private key and hasn\u0026rsquo;t been changed   Non-repudiation/Authentication Yes Since the message can be decrypted with Bob\u0026rsquo;s public key, it must have been encrypted with his private key    Man-in-the-middle scenario:\n The attacker intercepts the communication and receives the message from Bob. The attacker can\u0026rsquo;t decrypt the message as they can\u0026rsquo;t obtain Alice\u0026rsquo;s private key. The attacker can\u0026rsquo;t modify the message because if they do, Bob\u0026rsquo;s public key won\u0026rsquo;t work during decryption. The sender is guaranteed to be Bob as he signed the message with his private key, a key that attackers can\u0026rsquo;t obtain.  Summary and Comparison     Confidentiality Integrity Authentication Performance Man in the Middle     Asymmetric Encryption for Confidentiality: Without Digital Certificates Yes Yes No Fast Vulnerable   Asymmetric Encryption for Confidentiality: With Digital Certificates Yes Yes No Fast Safe   Asymmetric Encryption for Integrity and Authentication: Without Digital Signatures No Yes Yes Slow Vulnerable   Asymmetric Encryption for Integrity and Authentication: With Digital Signatures No Yes Yes Fast Vulnerable   Asymmetric Encryption for Integrity and Authentication: With Digital Certificates No Yes Yes Slow Vulnerable   Asymmetric Encryption for Confidentiality, Integrity and Authentication: With Digital Certificates Yes Yes Yes Slow Safe   Asymmetric Encryption for Confidentiality, Integrity and Authentication: With Digital Certificates and Signatures Yes Yes Yes Fast Safe    ","permalink":"https://www.serhatdundar.com/posts/symmetric-and-asymmetric-encryption/","summary":"Symmetric Encryption (Private Key Cryptography) In symmetric encryption only a single key, in other words, a private key is used to encrypt and decrypt a message. Symmetric encryption is also known as \u0026ldquo;Private Key Cryptography\u0026rdquo; as the whole encryption is only based on a private key.\nSome popular symetric encryption algorithms are:\n   Algorithm Cipher Key Size Block/State Size Popularity Notes     AES Block 128, 192, or 256 bits 128 bits 1 The best one.","title":"Symmetric and Asymmetric Encryption"},{"content":"Lambdas aren\u0026rsquo;t easy. This isn\u0026rsquo;t just a provocative start, but instead my overall experience planning, creating, and deploying them.\nLet\u0026rsquo;s be honest, making something up and running requires plenty of AWS knowledge. One might get lost easily even inside IAM alone.\nIAM groups, IAM users, IAM roles, IAM group policy attachments, IAM policy documents, IAM role policies - and how they connect is confusing enough considering what Lambda promises, simplicity. And this is just IAM.\nNowadays no infrastructure is simple. If you\u0026rsquo;re old enough to remember days when developers used to drag and drop files in an FTP agent to deploy something, things have changed a long time ago.\nThe rationale behind the change is not tools and it\u0026rsquo;s not Lambda to blame, it\u0026rsquo;s the scale. Even the simplest solutions, such as Lambda and S3, have designed to scale, and when you start working on things that can scale, they get complicated quickly.\nThink about the most basic Lambda + API Gateway + DynamoDB setup you can imagine. You will still need to learn a lot about AWS. Most likely you\u0026rsquo;re going to need an average understanding of six different services, at minimum. Lambda, API Gateway, S3, IAM, DynamoDB, CloudWatch and so on.\nThe number of development practices we follow has also evolved rapidly with the growing scale. Nowadays almost any software project has at least two environments (production, staging etc.), projects often deployed on multiple geographical locations, continuous integration and continuous deployment are everyday tools and deployment choices are rich, starting from manual deployments to semi-automated and fully-automated. Logging, monitoring and incident management are essential stages that need to be considered for each service. If you think about all the pieces one needs to combine to get a simple setup up and running and to reflect all these pieces on Terraform, or in any other IAC tool, the complexity can\u0026rsquo;t be underestimated.\nUse Case I started to investigate available Lambda deployment strategies while working on a hobby project. I\u0026rsquo;ve been using Lambdas for more than two years, but deployment internals has always been a mystery to me, as I was lucky to join a company with rich internal tooling is already available. However, of course, I knew about my options, such as SAM, Serverless Framework, Terraform and AWS SDKs.\nAt Babbel, the deployment of a Lambda is hassle-free, thanks to the internal tooling available. One can easily set up continuous deployment or choose to follow a semi-automated deployment strategy within a couple of hours with tools already available for this purpose.\nHowever, without tools to deal with deployment, it\u0026rsquo;s like Wild West out there. In this article, I will summarize some common deployment practices that I came across during my research. But first of all, here is the infrastructure setup I have:\n Github repositories managed by Terraform An AWS account fully managed by Terraform Two identical (resource-wise) environments for each AWS services, production and staging. IAM setup is near perfect, no services can do more than they supposed to. Service accesses, roles, policies and permissions are finely tuned. Log groups are organized clearly. Alarms and budgets set up for each service. A simple DynamoDB database to store names, such as \u0026ldquo;John Doe\u0026rdquo; An S3 bucket with versioning enabled to store lambda packages as .zip files. An API Gateway with a simple GET endpoint that is used as an event source for the lambda. Github Actions (CI) is enabled and it takes care of packing lambda functions and uploading them to the S3 bucket. The lambda function has an extensive test suite and tested both locally and on CI.  Here is how my demo system works:\n An HTTP GET request together with a query string parameter is sent to the API Gateway, i.e.: https://foo.com/names?name=John API Gateway triggers the lambda function. Lambda function queries the name in DynamoDB, and if a matching record has found in the database it returns 200 and a simple JSON in the body, otherwise, it returns 404 and an empty body.  The Next Step Briefly, I have a setup that I can test on AWS or locally. My Github Actions setup can pack the lambda and upload it to the S3 bucket. Now it\u0026rsquo;s time to decide on continuous deployment!\nThese are my acceptance criteria for Lambda deployments:\n CI should take care of lambda aliases and versioning. An IAM user dedicated to CI should only have minimum possible permissions, and should only access resources it actually manages. Changes reflected in main should be deployed to production lambda. Changes reflected in develop should be deployed to staging lambda. Deployments should be gradual on production. Deployments should be all at once on staging. CI should auto-rollback the deployment if a failure happens during the gradual deployment. CI should implement a health-check mechanism to see if the gradual deployment is successful. CI should take care of full roll-out when gradual deployment steps are successful. All CI actions should be runnable on local development environment without hassle.  With these criteria in mind, I started researching my options and best-practices out there.\nDeployments with Terraform I\u0026rsquo;ve listed 3 different approaches here. However, deployments with Terraform violates some criteria I\u0026rsquo;ve listed above. First of all, resources should be initialized and configured with Terraform, but Terraform shouldn\u0026rsquo;t take care of internal changes of them, according to the ideal infrastructure in my mind. For example, Terraform should bring a lambda to life, configure its resources such as memory and timeout, but shouldn\u0026rsquo;t care about the code changes of a lambda.\nIt makes sense, right? If not, think about S3. Terraform creates buckets, defined policies and lifecycles for them, but doesn\u0026rsquo;t care about what you put into those buckets. With a similar point of view, I didn\u0026rsquo;t prefer Terraform deployments for the following reasons:\n It just doesn\u0026rsquo;t feel right. Running a full terraform apply on CI requires an IAM user with extensive permissions. When multiple developers modify resources, keeping terraform tfstate in sync is a pain.  However, I will still list approaches I came across while exploring this option:\nApproach 1: Keep a .version file in your github repository This option is about keeping a .version file in the code repository located on Github to track changes on the code. Deployments are going to be possible after running terraform apply either locally or on CI.\nInitialize the file with Terraform:\nresource \u0026#34;github_repository_file\u0026#34; \u0026#34;name_searcher_lambda_version\u0026#34; { repository = github_repository.name_searcher_lambda.name branch = \u0026#34;main\u0026#34; file = \u0026#34;.version\u0026#34; content = \u0026#34;v0.0.1\u0026#34; commit_message = \u0026#34;Initialize the version file\u0026#34; commit_author = \u0026#34;Someone\u0026#34; commit_email = \u0026#34;something@gmail.com\u0026#34; } Add source_code_hash key to your lambda:\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;name_searcher\u0026#34; { function_name = \u0026#34;name-searcher\u0026#34; handler = \u0026#34;main\u0026#34; runtime = \u0026#34;go1.x\u0026#34; memory_size = 128 timeout = 5 s3_bucket = aws_s3_bucket.lambda_sources[each.key].bucket s3_key = \u0026#34;${github_repository.name_searcher_lambda.name}/index.zip\u0026#34; source_code_hash = github_repository_file.name_searcher_lambda_version.commit_sha } In this scenario, the lambda function is going to use the commit_sha of the .version file as source_code_hash. .version file needs to be updated whenever you would like to publish the lambda again. After the commit_sha has modified, you also need to run terraform apply to re-deploy the lambda from the S3 bucket.\nI didn\u0026rsquo;t like this approach at all.\nApproach 2: Store lambda packages in version-aware subfolders on S3 This approach is the one used in the official Terraform documentation for lambdas.\nIn this approach, you need to upload your lambda packages under versioned subfolders as follows:\ns3://lambda-sources/name-searcher/1.1.0/index.zip Then add the following to the variables.tf to get version number as a variable:\nvariable \u0026#34;lambda_version\u0026#34; {} Finally, add the lambda_version to the s3_key:\ns3_key = \u0026#34;v${var.lambda_version}/example.zip\u0026#34; Run terraform_apply with the lambda_version variable to deploy changes:\nterraform apply -var lambda_version=\u0026#34;1.0.1\u0026#34; For rolling back the deployment with an older version, follow the same logic:\nterraform apply -var lambda_version=\u0026#34;0.0.5\u0026#34; In this approach, you still need to take care of version numbers either locally or in your CI. Again, this approach requires too much manual intervention and violates my acceptance criteria.\nApproach 3: Upload .zip SHA as plaintext to S3 I\u0026rsquo;ve seen this approach in John Roach\u0026rsquo;s blog. It\u0026rsquo;s similar to the first two approaches but this one takes actual source code changes into consideration by getting the SHA of the whole deployment package for change tracking purposes.\nIn this approach, you need to upload a plaintext file including the SHA of the deployment package to the S3 bucket together with your .zip file. In the end, there will be two files in your S3 bucket for a lambda:\nlambda_package.zip # lambda deployment package lambda_package.zip.sha256 # plaintext file including SHA of the lambda_package.zip Then use this SHA as source_code_hash in your lambda:\ndata \u0026#34;aws_s3_bucket_object\u0026#34; \u0026#34;name_searcher_hash\u0026#34; { bucket = \u0026#34;lambda-sources\u0026#34; key = \u0026#34;path/name_searcher_payload.zip.base64sha256\u0026#34; } resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;name_searcher\u0026#34; { source_code_hash = data.aws_s3_bucket_object.name_searcher_hash.body s3_bucket = \u0026#34;lambda-sources\u0026#34; s3_key = \u0026#34;path/lambda_function_payload.zip\u0026#34; } The author of this approach prefers to ignore source_code_hash changes in lifecycle rules (which makes sense) and re-deploys the lambda function as follows:\naws lambda update-function-code --function-name name_searcher \\  --s3-bucket lambda-sources \\  --s3-key path/lambda_function_payload.zip This approach can also be combined with the first two approaches. You can treat commit SHAs similar to versions, and vice versa, it all depends on your choices. I only provided a modified brief version of this approach, so visit the original post for more details.\nDeployments with SAM SAM specification is great to make things up and running quickly, especially if your infrastructure isn\u0026rsquo;t complex. For the use case I mentioned earlier SAM could be an option, however, there are things I don\u0026rsquo;t like about SAM:\n It just doesn\u0026rsquo;t feel right when used together with Terraform. SAM doesn\u0026rsquo;t support some AWS services such as IAM Role, IAM Policy, KMS etc. This forces infrastructure to split between Terraform and SAM, and I want a single source of truth, Terraform. SAM makes more sense and provides convenience especially when used together with other services such as API Gateway, however, I prefer to keep them in Terraform. It\u0026rsquo;s a little bit magical! I don\u0026rsquo;t feel comfortable SAM creating IAM roles and permission automatically for me. Programmatic access and structuring in SAM templates are ugly. Plenty of IAM permissions needs to be granted to run sam deploy on CI that again I prefer to avoid.  Because of the listed reasons, I decided not to evaluate SAM as an option.\nDeployments with Serverless Serverless and SAM are somehow similar, both are CloudFormation abstractions, and both generate CloudFormation templates. For the use case I mentioned, Serverless again didn\u0026rsquo;t feel right for the same reasons I\u0026rsquo;ve listed for SAM.\nDeployments with AWS SDK AWS SDK is at the core of AWS services. The amount of functionality they provide enables developers to build their custom solutions on top of what is already available. Therefire, I\u0026rsquo;ve adopted AWS SDK and built my own deployment workflow satisfying all acceptance criteria I\u0026rsquo;ve defined. Here are the details:\nAn IAM user for CI usage with policies (aws_iam_policy_document) including minimum possible permissions:\n# Only for the bucket that CI can upload \u0026#34;s3:Get*\u0026#34; \u0026#34;s3:List*\u0026#34; \u0026#34;s3:PutObject\u0026#34; \u0026#34;s3:PutObjectAcl\u0026#34;# Only for the lambda that CI can deploy \u0026#34;lambda:GetFunction\u0026#34; \u0026#34;lambda:GetFunctionConfiguration\u0026#34; \u0026#34;lambda:InvokeFunction\u0026#34; \u0026#34;lambda:UpdateFunctionCode\u0026#34; \u0026#34;lambda:PublishVersion\u0026#34; \u0026#34;lambda:UpdateAlias\u0026#34; Credentials of this IAM user are being added to the Github Repository of lambda as secrets by Terraform. The IAM user of CI can only upload files to the S3 lambda_sources S3 bucket, and can\u0026rsquo;t access others. Similarly, it can only deploy the related lambda, not others.\nThere is an alias that is going to be exposed for client usage, named live. The ARN of the alias I\u0026rsquo;ve created is the one to be triggered from API Gateway:\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;name_searcher\u0026#34; { for_each = var.environments function_name = \u0026#34;name-searcher-${each.key}\u0026#34; description = \u0026#34;Queries the harmful-domains DynamoDB table\u0026#34; handler = \u0026#34;main\u0026#34; runtime = \u0026#34;go1.x\u0026#34; memory_size = 128 timeout = 5 s3_bucket = aws_s3_bucket.lambda_sources[each.key].bucket s3_key = \u0026#34;${github_repository.name_searcher_lambda.name}/v1.0.0/index.zip\u0026#34; role = aws_iam_role.lambda_name_searcher.arn } resource \u0026#34;aws_lambda_alias\u0026#34; \u0026#34;name_searcher\u0026#34; { for_each = var.environments name = \u0026#34;live\u0026#34; description = \u0026#34;Live version of the lambda. Gradual traffic shifting in place.\u0026#34; function_name = aws_lambda_function.name_searcher[each.key].arn function_version = \u0026#34;$LATEST\u0026#34; lifecycle { ignore_changes = [function_version] } } ignore_changes part here is important because I want CI to manage releases.\nThe rest is pretty standard. An S3 bucket, some Terraform config for managing the Github repository and so on.\nAfter creating the initial infrastructure I\u0026rsquo;ve decided to create a Go module to take care of canary Lambda deployments. Therefore, I\u0026rsquo;ve created a module around AWS SDK and integrated into the CI setup of all my personal lambda projects.\nHere are you can have a look:\n  https://github.com/msdundar/kanarya\n  References  https://learn.hashicorp.com/tutorials/terraform/lambda-api-gateway https://johnroach.io/2020/09/04/deploying-lambda-functions-with-terraform-just-dont/ https://medium.com/galvanize/aws-lambda-deployment-with-terraform-24d36cc86533 https://docs.aws.amazon.com/sdk-for-go/api/service/lambda/  ","permalink":"https://www.serhatdundar.com/posts/canary-lambda-deployments/","summary":"Lambdas aren\u0026rsquo;t easy. This isn\u0026rsquo;t just a provocative start, but instead my overall experience planning, creating, and deploying them.\nLet\u0026rsquo;s be honest, making something up and running requires plenty of AWS knowledge. One might get lost easily even inside IAM alone.\nIAM groups, IAM users, IAM roles, IAM group policy attachments, IAM policy documents, IAM role policies - and how they connect is confusing enough considering what Lambda promises, simplicity. And this is just IAM.","title":"Canary Lambda Deployments"},{"content":"Title: The Design of Web APIs ISBN: 9781617295102 Publisher: Manning WWW: https://www.manning.com/books/the-design-of-web-apis Pages: 389 I recently had a chance to read \u0026ldquo;The Design of Web APIs\u0026rdquo; from Manning Publishing. The book is inspired by \u0026ldquo;The Design of Everyday Things\u0026rdquo;, the legendary writing of Donald Norman, that I still own as an original hard-copy printed in 1990. I like this masterpiece not because I studied Donald Norman quite a lot during my PhD studies (especially during the Human-Computer Interaction course), but also it\u0026rsquo;s still very relevant writing.\nCommentary The book provides a broad view of the decision-making process of API design with rich visualizations and comprehensive content in a beginner-friendly tone. However, it can also be useful for seasoned developers.\n Men do not stumble over mountains but over molehills. Confucius.\n The book covers a wide range of topics, starting from HTTP and REST API basics, to more complex topics such as decision making between REST, gRPC, and GraphQL. Inexperienced engineers can benefit from the book to strengthen their fundamental knowledge, and experienced engineers who are often expected to prepare guidelines, define standards and pick best practices for various topics (including APIs), can benefit from the book as a reference resource.\nFundamentals As someone who frequently sits in the interviewer chair, I came across many candidates who aren\u0026rsquo;t comfortable enough about the fundamentals web development, such as HTTP, REST and APIs. The fact is, nowadays, web frameworks hide a great deal of complexity running behind them, and especially engineers who got into coding by using a web development framework often fall into the convenience trap and miss some fundamentals. If you are a software engineer got into the industry with some web frameworks such as React, Rails, VueJS etc. this book can potentially uncover details that your framework hides.\nMore years I spend in tech, the easier it becomes to ask fundamental questions. Intimidated by the know-it-all types in the earlier years, I always assumed it\u0026#39;d be the opposite.\n\u0026mdash; Jaana Dogan ヤナ ドガン (@rakyll) September 13, 2020  Fundamentals don\u0026rsquo;t get old. Frameworks, languages and tools come and go, and usually faster than your pace of learning, but fundamental knowledge stays with you. HTTP is 32 years old, and it\u0026rsquo;s not going anywhere soon. REST is 25 years old and it is still widely used. On the other hand, I\u0026rsquo;ve been professionally programming for 11 years and I already changed tens of languages and frameworks, historically Classic ASP (not .NET!), PHP, Python, Ruby - and nowadays GoLang. My suggestion for inexperienced engineers would be to focus on fundamentals as early as possible in their careers, that will help them to become better decision-makers in the future.\nClosing Words Overall, I enjoyed the book! It\u0026rsquo;s a comprehensive resource for anyone interested in API design, independent of experience level. Experienced engineers can toss off the reading in a couple of days, while engineers new in the field can take their time and benefit from the rich content and visualizations.\nCheers.\n","permalink":"https://www.serhatdundar.com/posts/book-review-the-design-of-web-apis/","summary":"Title: The Design of Web APIs ISBN: 9781617295102 Publisher: Manning WWW: https://www.manning.com/books/the-design-of-web-apis Pages: 389 I recently had a chance to read \u0026ldquo;The Design of Web APIs\u0026rdquo; from Manning Publishing. The book is inspired by \u0026ldquo;The Design of Everyday Things\u0026rdquo;, the legendary writing of Donald Norman, that I still own as an original hard-copy printed in 1990. I like this masterpiece not because I studied Donald Norman quite a lot during my PhD studies (especially during the Human-Computer Interaction course), but also it\u0026rsquo;s still very relevant writing.","title":"Book Review: The Design of Web APIs"},{"content":"Grunt is a Javascript task runner. It automates repetitive tasks like minification, compilation, unit testing, linting, etc. So it\u0026rsquo;s also quite useful for packing JS projects for AWS Lambda.\nInstall the CLI globally:\nnpm install -g grunt-cli Add it to the package.json:\nnpm install grunt --save-dev The Gruntfile.js or Gruntfile.coffee file is a valid JavaScript or CoffeeScript file that belongs in the root directory of your project.\nA Gruntfile is comprised of the following parts:\n The \u0026ldquo;wrapper\u0026rdquo; function Project and task configuration Loading Grunt plugins and tasks Custom tasks  A sample Gruntfile is as follows:\nmodule.exports = function (grunt) { // load all grunt plugins  require(\u0026#34;load-grunt-tasks\u0026#34;)(grunt); // variables  var buildPath = \u0026#34;./tmp/build/\u0026#34;; var destFile = \u0026#34;./dest/index.zip\u0026#34; grunt.initConfig({ copy: { build: { src: [ \u0026#34;./src/**\u0026#34;, \u0026#34;./index.js\u0026#34;, \u0026#34;./package-lock.json\u0026#34;, \u0026#34;./package.json\u0026#34;, \u0026#34;./client_config.json\u0026#34;, ], dest: buildPath, }, }, exec: { build: { cwd: buildPath, cmd: \u0026#34;npm ci --production\u0026#34;, }, }, zip: { build: { cwd: buildPath, src: buildPath + \u0026#34;**\u0026#34;, dest: destFile, compression: \u0026#34;DEFLATE\u0026#34;, }, }, }); // default task  grunt.registerTask(\u0026#34;default\u0026#34;, [\u0026#34;build\u0026#34;]); // build task  grunt.registerTask(\u0026#34;build\u0026#34;, [\u0026#34;copy\u0026#34;, \u0026#34;exec\u0026#34;, \u0026#34;zip\u0026#34;]); }; package.json can then be configured for npm build:\n\u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;grunt\u0026#34; } Cheers.\n","permalink":"https://www.serhatdundar.com/posts/packing-js-projects-for-aws-lambda-with-grunt/","summary":"Grunt is a Javascript task runner. It automates repetitive tasks like minification, compilation, unit testing, linting, etc. So it\u0026rsquo;s also quite useful for packing JS projects for AWS Lambda.\nInstall the CLI globally:\nnpm install -g grunt-cli Add it to the package.json:\nnpm install grunt --save-dev The Gruntfile.js or Gruntfile.coffee file is a valid JavaScript or CoffeeScript file that belongs in the root directory of your project.\nA Gruntfile is comprised of the following parts:","title":"Packing JS projects for AWS Lambda with Grunt"},{"content":"During nokul we\u0026rsquo;ve heavily implemented PostgreSQL features into our Rails application. Unfortunately, ActiveRecord doesn\u0026rsquo;t come with constraint support for PostgreSQL, but rein does a fantastic job covering what\u0026rsquo;s missing in ActiveRecord. We believe that, one shouldn\u0026rsquo;t rely on a web application, that is very prone for human-error, when it comes to data integrity. Therefore our PostgreSQL tables included various constraints and limits.\nBelow you will find a set of rules that we\u0026rsquo;ve investigated, implemented and battle tested with various types.\n String Type   Do not use limit: N in migrations for string type. Instead, use add_length_constraint.\n  If you aren\u0026rsquo;t sure about the length of a string field, add 255 limit in the constraint, similar to the MySQL approach:\nadd_length_constraint :table, :column, less_than_or_equal_to: 255   If length of an attribute is constant, consider using equal_to:\nadd_length_constraint :users, :id_number, equal_to: 11   Do not use text type in migrations. Instead use string type and add_length_constraint with 65535 limit, similar to the MySQL approach. This will protect you from cells with crazy amount of data:\nadd_length_constraint :decisions, :description, less_than_or_equal_to: 65535   Use presence and unique constraints whenever necessary:\nadd_presence_constraint :countries, :name add_unique_constraint :users, :email    varchar VS varchar(n) VS char VS text  PostgreSQL doesn\u0026rsquo;t have a limit for string type by default. Technically, one can insert gigabytes of data into a string field, therefore we\u0026rsquo;ve adopted a hard limit, 65535, similar to the MySQL approach. There isn\u0026rsquo;t a performance or size difference between varchar (n) and text in PostgreSQL. varchar(n) LOCKs the table in case of a change, that often causes downtimes. Therefore limits shouldn\u0026rsquo;t be added to the column, instead they should be added as a CHECK.   Integer Type   Do not use limit: N in migrations for integer type.\n  Integers attributes often expected to return 0, instead of nil in case of non-existence. Therefore, add a null_constraint if you don\u0026rsquo;t have a good reason not to:\nadd_null_constraint :users, :articles_count   Defining a default value (often 0) will make sense most of the time. Also add a null_constraint together with the default value:\nt.integer :articles_count, default: 0 add_null_constraint :users, :articles_count   Add a numericality_constraint constraint, if you aren\u0026rsquo;t planning to accept a negative value in the column:\nadd_numericality_constraint :users, :articles_count, greater_than_or_equal_to: 0   For numbers with exact upper and lower bounds, add numericality_constraint:\nadd_numericality_constraint :articles, :month, greater_than_or_equal_to: 1, less_than_or_equal_to: 12 add_numericality_constraint :articles, :year, greater_than_or_equal_to: 1950, less_than_or_equal_to: 2050    Float Type   float: Useful when accuracy isn\u0026rsquo;t very important and when you\u0026rsquo;re only interested in 3-5 numbers after the comma. Also useful when you are running complex arithmetic with these numbers.\n  decimal: Useful when accuracy is very important (as in money), even more important than the performance.\n  For both types, always add null_constraint if you\u0026rsquo;ve defined a default value:\nt.decimal :min_credit, precision: 5, scale: 2, default: 0 add_null_constraint :course_types, :min_credit   For both types, add a numericality_constraint if you aren\u0026rsquo;t accepting negative values:\nadd_numericality_constraint :course_types, :min_credit, greater_than_or_equal_to: 0   Often a float or decimal attribute isn\u0026rsquo;t expected to return nil in case of non-existence, instead, they\u0026rsquo;re expected to return 0. Therefore, add a null_constraint if you don\u0026rsquo;t have a good reason not to do so:\nadd_null_constraint :course_types, :min_credit    Float \u0026amp; Decimal There are some differences between float and decimal in PostgreSQL. First of all, let\u0026rsquo;s start with checking what Rails produces for each type:\nt.float :incentive_point | Column | Type | Nullable | | --------------- | ---------------- | -------- | | incentive_point | double precision | | t.decimal :min_credit, precision: 5, scale: 2 | Column | Type | Nullable | | ------ | ------------ | -------- | | credit | numeric(5,2) | | So, types in Rails converted into the following types in PostgreSQL:\n float -\u0026gt; double_precision decimal -\u0026gt; numeric(x, y) \u0026amp; decimal(x, y)  If we dig into these types in PostgreSQL:\n| name | size | description | range | in-rails | | ---------------- | -------- | ----------- | --------------------------------------------------------------------------- | -------- | | decimal (p, s) | variable | exact | p(total digits), s(digits after decimal point), max(p)=131072, max(s)=16383 | decimal | | numeric (p, s) | variable | exact | p(total digits), s(digits after decimal point), max(p)=131072, max(s)=16383 | decimal | | double-precision | 8-bytes | inexact | 15 significant digits, unlimited size | float | Briefly:\n decimal and numeric types are the same in PostgreSQL. decimal and numeric types are exact, but double-precision is inexact. decimal and numeric types have some limits, while double-precision can be unlimited.  Exact? exact types store the data as it\u0026rsquo;s submitted, however inexact types aren\u0026rsquo;t. Therefore, numberic type should be preferred in cases where precision is important.\n Boolean Type   Always add a null_constraint for boolean columns:\nadd_null_constraint :students, :active   Always add a default value for boolean columns:\nt.boolean :students, active: false   A boolean field often isn\u0026rsquo;t expected to return nil. For example:\nStudent.where(active: nil).count =\u0026gt; 50 Student.where(active: false).count =\u0026gt; 40 Student.where(active: true).count =\u0026gt; 60 In this case, it\u0026rsquo;s very hard to tell how many students are active, and how many of them aren\u0026rsquo;t. The data shown above needs to be corrected to before analyzed. Therefore, stick with the two rules explained here.\n Reference Type   Use null: false for foreign_key columns, if there is no optional: true relation in between:\n  Add foreign_key constraint to ensure referential integrity:\nt.references :unit, null: false, foreign_key: true   Add foreign_key constraints directly to the column, instead of as a CHECK, as these columns typically not expected to change very often.\n   References  https://www.depesz.com/2010/03/02/charx-vs-varcharx-vs-varchar-vs-text/ https://stackoverflow.com/questions/4848964/postgresql-difference-between-text-and-varchar-character-varying https://dba.stackexchange.com/questions/125499/what-is-the-overhead-for-varcharn/125526#125526 https://dba.stackexchange.com/questions/89429/would-index-lookup-be-noticeably-faster-with-char-vs-varchar-when-all-values-are/89433#89433 https://gist.github.com/icyleaf/9089250 https://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/SchemaStatements.html https://github.com/rails/rails/blob/master/activerecord/lib/active_record/connection_adapters/postgresql_adapter.rb https://stackoverflow.com/questions/38053596/benchmark-bigint-vs-int-on-postgresql https://stackoverflow.com/questions/2966524/calculating-and-saving-space-in-postgresql/7431468#7431468 http://forums.devshed.com/postgresql-help-21/numeric-vs-float-607281.html https://stackoverflow.com/a/20887107/818033 https://www.linuxtopia.org/online_books/database_guides/Practical_PostgreSQL_database/PostgreSQL_x2632_004.htm https://stackoverflow.com/a/8523253/818033  ","permalink":"https://www.serhatdundar.com/posts/rails-and-postgresql-types/","summary":"During nokul we\u0026rsquo;ve heavily implemented PostgreSQL features into our Rails application. Unfortunately, ActiveRecord doesn\u0026rsquo;t come with constraint support for PostgreSQL, but rein does a fantastic job covering what\u0026rsquo;s missing in ActiveRecord. We believe that, one shouldn\u0026rsquo;t rely on a web application, that is very prone for human-error, when it comes to data integrity. Therefore our PostgreSQL tables included various constraints and limits.\nBelow you will find a set of rules that we\u0026rsquo;ve investigated, implemented and battle tested with various types.","title":"Rails and PostgreSQL: Types"},{"content":"During nokul we\u0026rsquo;ve heavily implemented PostgreSQL features into our Rails application. Unfortunately, ActiveRecord doesn\u0026rsquo;t come with constraint support for PostgreSQL, but rein does a fantastic job covering what\u0026rsquo;s missing in ActiveRecord. We believe that, one shouldn\u0026rsquo;t rely on a web application, that is very prone for human-error, when it comes to data integrity. Therefore our PostgreSQL tables included various constraints and limits.\nBelow you will find a set of rules that we\u0026rsquo;ve investigated, implemented and battle tested with various types.\nNot Null Constraint \u0026amp; Presence Constraint Use null: false for foreign_key columns, if there is no optional: true relation in between.\nt.references :unit, null: false, foreign_key: true Do not use null: false for integer, boolean and float types, instead use add_null_constraint:\nadd_null_constraint :students, :active Do not use null: false for string type, instead use add_presence_constraint:\nadd_presence_constraint :countries, :name  Not Null Constraint vs. Not Null Check It\u0026rsquo;s possible to define NOT NULL case as a CONSTRAINT or as a CHECK, in PostgreSQL. However, there are some differences between them:\n change_column_null (or null: false) adds a CONSTRAINT on a column, add_presence_constraint adds a CHECK to the table.   change_column_null (null: false) change_column_null method of Rails adds a not null CONSTRAINT to column:\nchange_column_null :cities, :country_id, false Table \u0026#34;public.cities\u0026#34; Column | Type | Collation | Nullable | Default --------------+------------------------+-----------+----------+------------------------------------ id | bigint | | not null | nextval(\u0026#39;cities_id_seq\u0026#39;::regclass) name | character varying(255) | | not null | country_id | bigint | | not null | Indexes: \u0026#34;cities_pkey\u0026#34; PRIMARY KEY, btree (id) \u0026#34;cities_name_unique\u0026#34; UNIQUE CONSTRAINT, btree (name) DEFERRABLE \u0026#34;index_cities_on_country_id\u0026#34; btree (country_id)  add_null_constraint add_null_constraint method of rein adds a not_null CHECK to table:\nadd_null_constraint :cities, :country_id Table \u0026#34;public.cities\u0026#34; Column | Type | Collation | Nullable | Default --------------+------------------------+-----------+----------+------------------------------------ id | bigint | | not null | nextval(\u0026#39;cities_id_seq\u0026#39;::regclass) name | character varying(255) | | not null | country_id | bigint | | | Indexes: \u0026#34;cities_pkey\u0026#34; PRIMARY KEY, btree (id) \u0026#34;cities_name_unique\u0026#34; UNIQUE CONSTRAINT, btree (name) DEFERRABLE \u0026#34;index_cities_on_country_id\u0026#34; btree (country_id) Check constraints: \u0026#34;cities_country_id_null\u0026#34; CHECK (country_id IS NOT NULL) PostgreSQL documentation explains how NOT NULL CHECK and NOT NULL CONSTRAINT are similar, but NOT NULL CONSTRAINT is faster:\n A not-null constraint is always written as a column constraint. A not-null constraint is functionally equivalent to creating a check constraint CHECK (column_name IS NOT NULL), but in PostgreSQL creating an explicit not-null constraint is more efficient.\n Unfortunately the official PostgreSQL documentation doesn\u0026rsquo;t tell the performance difference in numbers. However, a Stack Overflow user mentions the performance difference as quite insignificant, around 0.5%.\n add_presence_constraint add_presence_constraint is used to check the existence of strings. It doesn\u0026rsquo;t allow empty strings as add_null_constraint does:\nUser.create(email: \u0026#39; \u0026#39;) # can\u0026#39;t be created when add_presence_constraint is in place  Why CHECK instead of CONSTRAINT? While the official PostgreSQL documentation mentions the performance difference between CHECK and CONSTRAINT in favor of CONSTRAINT, rein still adds a CHECK to satisfy NOT NULL condition, simply because of two reasons:\n Reverting a CHECK is easy, but a whole column needs to be rewritten when reverting a CONSTRAINT. Since a whole column needs to be re-written when there is a change in CONSTRAINT, an AccessExclusiveLock added to the table. On the other hand, changes on CHECK doesn\u0026rsquo;t add an AccessExclusiveLock to tables, so that they don\u0026rsquo;t cause down times.   Unique Constraint unique_constraint works a little bit different than the others, it adds an index:\nusers_email_unique users_id_number_unique If you want a unique_constraint to work as the latest step of a transaction (for performance reasons), you can also defer it:\nadd_unique_constraint :books, :isbn, deferred: true  References  https://www.postgresql.org/docs/current/ddl-constraints.html#id-1.5.4.5.6 https://dba.stackexchange.com/a/158644  ","permalink":"https://www.serhatdundar.com/posts/rails-and-postgresql-constraints/","summary":"During nokul we\u0026rsquo;ve heavily implemented PostgreSQL features into our Rails application. Unfortunately, ActiveRecord doesn\u0026rsquo;t come with constraint support for PostgreSQL, but rein does a fantastic job covering what\u0026rsquo;s missing in ActiveRecord. We believe that, one shouldn\u0026rsquo;t rely on a web application, that is very prone for human-error, when it comes to data integrity. Therefore our PostgreSQL tables included various constraints and limits.\nBelow you will find a set of rules that we\u0026rsquo;ve investigated, implemented and battle tested with various types.","title":"Rails and PostgreSQL: Constraints"},{"content":"Configuration File  RAM amounts defined below doesn\u0026rsquo;t represent the total amount of memory available for your machine, instead, it represents what PostgreSQL can use maximum. Decribed configuration requires PostgreSQL \u0026gt;= 10. Remove max_parallel_workers for older versions.  The configuration file is usually located at /etc/postgresql/$VERSION/main/postgresql.conf in Debian-derivatives. For locating the file in other operating systems, you can use psql as follows:\npsql -U postgres show config_file; Tracking Query Statistics Enable the pg_stat extension, if you are interested in tracking query statistics:\nshared_preload_libraries = \u0026#39;pg_stat_statements\u0026#39; pg_stat_statements.track = all Then restart postgres:\nservice postgresql restart Sample Configuration # Postgresql Version: 10 || 11 # OS Type: linux # Total Memory (RAM): 8 GB # CPUs num: 2 # Connections num: 200 # Data Storage: ssd max_connections = 200 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.7 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 11MB min_wal_size = 1GB max_wal_size = 2GB max_worker_processes = 2 max_parallel_workers_per_gather = 1 max_parallel_workers = 2 References  https://pgtune.leopard.in.ua/#/  ","permalink":"https://www.serhatdundar.com/posts/tuning-postgresql-configuration/","summary":"Configuration File  RAM amounts defined below doesn\u0026rsquo;t represent the total amount of memory available for your machine, instead, it represents what PostgreSQL can use maximum. Decribed configuration requires PostgreSQL \u0026gt;= 10. Remove max_parallel_workers for older versions.  The configuration file is usually located at /etc/postgresql/$VERSION/main/postgresql.conf in Debian-derivatives. For locating the file in other operating systems, you can use psql as follows:\npsql -U postgres show config_file; Tracking Query Statistics Enable the pg_stat extension, if you are interested in tracking query statistics:","title":"Tuning PostgreSQL Configuration"},{"content":"The most recent versions (5.1 and 5.2) of Ruby on Rails has shipped with a new feature named as encrypted credentials which replaces the secrets.yml, and enables you to keep sensitive data in an encrypted file named as config/credentials.yml.enc.\nHowever, this feature only works with a single file that is config/credentials.yml.enc. Recently we needed to add some data in our repository, which we wanted to keep as encrypted, but that also didn\u0026rsquo;t really fit into the credentials.yml.enc conceptually.\n We were aware of many great tools available for encrypting files and decrypting them when needed, but we didn\u0026rsquo;t want to implement another complexity level or some other dependency (such as a GEM or an infra tool) to the app. Therefore we aimed to solve this problem with minimum available tools, and preferably, with core capabilities of Ruby and Rails.\n After taking a look to Rails source code, roktas and I\u0026rsquo;ve decided to imitate the behaviour of encrypted credentials, and we ended up writing a simple wrapper around the core ActiveSupport methods:\n# frozen_string_literal: true module FileEncryptor DEFAULT_PARAMS = { env_key: \u0026#39;RAILS_MASTER_KEY\u0026#39;, key_path: Rails.root.join(\u0026#39;config\u0026#39;, \u0026#39;master.key\u0026#39;), raise_if_missing_key: true }.freeze def self.encrypt(path) encryptor = ActiveSupport::EncryptedFile.new( merge_with_content_path(Rails.root.join(\u0026#39;db\u0026#39;, \u0026#39;encrypted_data\u0026#39;, path.split(\u0026#39;/\u0026#39;).last + \u0026#39;.enc\u0026#39;)) ) encryptor.write(File.read(Rails.root.join(path))) end def self.decrypt(path) encryptor = ActiveSupport::EncryptedFile.new( merge_with_content_path(Rails.root.join(path)) ) encryptor.read end def self.decrypt_lines(path) decrypt(path).split(\u0026#34;\\n\u0026#34;) end def self.merge_with_content_path(value) DEFAULT_PARAMS.merge( content_path: value ) end end Briefly, it wraps the ActiveSupport::EncryptedFile and behaves exactly the same. You can use either an environment variable or the master.key for decrypting encrypted files, and it works with absolute and relative paths. Here are some examples:\nEncrypt a file:\nFileEncryptor.encrypt(\u0026#39;db/static_data/users.csv\u0026#39;) Decrypt an encrypted file as a whole:\nFileEncryptor.decrypt(\u0026#39;db/encrypted_data/users.csv.enc\u0026#39;) Decrypt an encrypted file as an array:\nFileEncryptor.decrypt_lines(\u0026#39;db/encrypted_data/users.csv.enc\u0026#39;) decrypt_lines method allows you to iterate encrypted records one by one:\nusers = FileEncryptor.decrypt_lines(\u0026#39;/lib/important_files/users.csv.enc\u0026#39;) users.each do |user| User.create(...) end No magic, no dependencies, no GEMs, no packages. Just native capabilities of Ruby on Rails.\nCheers.\n","permalink":"https://www.serhatdundar.com/posts/encrypting-sensitive-data-with-rails/","summary":"The most recent versions (5.1 and 5.2) of Ruby on Rails has shipped with a new feature named as encrypted credentials which replaces the secrets.yml, and enables you to keep sensitive data in an encrypted file named as config/credentials.yml.enc.\nHowever, this feature only works with a single file that is config/credentials.yml.enc. Recently we needed to add some data in our repository, which we wanted to keep as encrypted, but that also didn\u0026rsquo;t really fit into the credentials.","title":"Encrypting Sensitive Data With Rails"},{"content":"Here is a sample Ruby on Rails fixture named as newsletter:\nnewsletter: name: foo message: bar first_name: foo last_name: bar There are two popular ways to use this fixture in your Rails tests. The first one is directly calling the name of fixture file, followed by a symbol stating the name of any individual fixture:\nclass NewsletterTest \u0026lt; ActiveSupport::TestCase test \u0026#39;a sample test\u0026#39; do assert newsletters(:newsletter).valid? end end And the other one is assigning fixtures to an instance variable in the setup block:\nclass NewsletterTest \u0026lt; ActiveSupport::TestCase setup do @newsletter = newsletters(:tenant_newsletter) end test \u0026#39;a sample test\u0026#39; do assert @newsletter.valid? end end When you would like to stick with instance variables, there is an easier way, instantiated fixtures:\nclass NewsletterTest \u0026lt; ActiveSupport::TestCase self.use_instantiated_fixtures = true test \u0026#39;a sample test\u0026#39; do assert @newsletter.valid? end end instantiated_fixtures automatically creates instance variables for each fixture item, and they become accessible as instance variables with the same name. Imagine a more populated fixture like this:\ncity: name: Samsun country: name: Turkey region: name: Black Sea when you enable instantiated_fixtures you will automatically have three different instance variables named as @city, @country and @region. These instance variables will immediately be accessible from anywhere in your test file.\nThis behaviour might come in handy for some cases but please be aware of the risk of conflicting variables! Since instantiated_fixtures will give you an instance variable for each item, you must make sure that you don\u0026rsquo;t use these variable names anywhere else in your test files, in order to prevent possible conflicts and confusions.\nCheers.\n","permalink":"https://www.serhatdundar.com/posts/rails-instantiated-fixtures/","summary":"Here is a sample Ruby on Rails fixture named as newsletter:\nnewsletter: name: foo message: bar first_name: foo last_name: bar There are two popular ways to use this fixture in your Rails tests. The first one is directly calling the name of fixture file, followed by a symbol stating the name of any individual fixture:\nclass NewsletterTest \u0026lt; ActiveSupport::TestCase test \u0026#39;a sample test\u0026#39; do assert newsletters(:newsletter).valid? end end And the other one is assigning fixtures to an instance variable in the setup block:","title":"Rails Instantiated Fixtures"},{"content":"I recently had to import some quite large SQL dumps to my local machine for data analysis purposes. There were 10 different .sql dumps in total, and each of them were sized more than 100GB.\nNaive Attempt First of all, I tried to import each by using the \u0026lt; operator:\nmysql some_database \u0026lt; some_dump.sql Unsurprisingly, each task took very long to finish, approximately 6-7 hours per dump file, in a brand new laptop with i7 CPU, 16GB RAM and 1TB SSD. Since I didn\u0026rsquo;t have 60 hours for importing all, I had to take a look for solutions that can potentially speed up the process.\nConfiguration After Googling and reading through a set of similar issues on Stackoverflow, I came across an answer, referencing to Vadim Tkachenko and explaining why it was taking so long to import.\nAfter grasping the reasons behind, I changed the InnoDB settings (my.ini) as follows:\ninnodb_buffer_pool_size = 12G # 60% - 70% of your RAM size innodb_log_buffer_size = 16M # 16M or 32M is fine innodb_log_file_size = 3G # 25% of buffer pool size innodb_write_io_threads = 32 # 32 is fine, 64 is maximum innodb_flush_log_at_trx_commit = 0 That was it after restarting MySQL:\n$ sudo service mysql restart Results New settings were clearly effective on the import time of dump files. Each import task started to take around 20-30 minutes after fine tuning the settings.\nCheers.\n","permalink":"https://www.serhatdundar.com/posts/import-big-mysql-databases-faster/","summary":"I recently had to import some quite large SQL dumps to my local machine for data analysis purposes. There were 10 different .sql dumps in total, and each of them were sized more than 100GB.\nNaive Attempt First of all, I tried to import each by using the \u0026lt; operator:\nmysql some_database \u0026lt; some_dump.sql Unsurprisingly, each task took very long to finish, approximately 6-7 hours per dump file, in a brand new laptop with i7 CPU, 16GB RAM and 1TB SSD.","title":"Import Big MySQL Databases Faster"},{"content":"Rake tasks in a loop, will only executed once if they are not \u0026ldquo;re-enabled\u0026rdquo;. Take a look at this example:\nnamespace :yoksis do desc \u0026#39;fetches all references\u0026#39; task :references do mapping = { get_instruction_language: \u0026#39;UnitInstructionLanguage\u0026#39;, get_instruction_type: \u0026#39;UnitInstructionType\u0026#39; } mapping.each do |action, klass| Rake::Task[\u0026#39;yoksis:reference\u0026#39;].invoke(action, klass) end end desc \u0026#39;fetch an individual reference\u0026#39; task :reference, %i[soap_method klass] =\u0026gt; [:environment] do |_, args| puts args[:soap_method] puts args[:klass] end end When you run the yoksis:references task, it will only print out {get_instruction_language: 'UnitInstructionLanguage'} and will skip the second item of the mapping hash.\nStrange, isn\u0026rsquo;t it? To overcome this issue, the rake task needs to be reenabled during the iteration:\nRake::Task[\u0026#39;yoksis:reference\u0026#39;].reenable So the previous task becomes:\nnamespace :yoksis do desc \u0026#39;fetches all references\u0026#39; task :references do mapping = { get_instruction_language: \u0026#39;UnitInstructionLanguage\u0026#39;, get_instruction_type: \u0026#39;UnitInstructionType\u0026#39; } mapping.each do |action, klass| Rake::Task[\u0026#39;yoksis:reference\u0026#39;].invoke(action, klass) Rake::Task[\u0026#39;yoksis:reference\u0026#39;].reenable end end desc \u0026#39;fetch an individual reference\u0026#39; task :reference, %i[soap_method klass] =\u0026gt; [:environment] do |_, args| puts args[:soap_method] puts args[:klass] end end Cheers.\n","permalink":"https://www.serhatdundar.com/posts/running-rake-tasks-in-a-loop/","summary":"Rake tasks in a loop, will only executed once if they are not \u0026ldquo;re-enabled\u0026rdquo;. Take a look at this example:\nnamespace :yoksis do desc \u0026#39;fetches all references\u0026#39; task :references do mapping = { get_instruction_language: \u0026#39;UnitInstructionLanguage\u0026#39;, get_instruction_type: \u0026#39;UnitInstructionType\u0026#39; } mapping.each do |action, klass| Rake::Task[\u0026#39;yoksis:reference\u0026#39;].invoke(action, klass) end end desc \u0026#39;fetch an individual reference\u0026#39; task :reference, %i[soap_method klass] =\u0026gt; [:environment] do |_, args| puts args[:soap_method] puts args[:klass] end end When you run the yoksis:references task, it will only print out {get_instruction_language: 'UnitInstructionLanguage'} and will skip the second item of the mapping hash.","title":"Running Rake Tasks in a Loop"},{"content":"has_many :through is a useful association type of Rails. It\u0026rsquo;s mostly popular and often used as a join model for many-to-many relations.\nHowever, has_many :through is more than a simple join model, because it conducts INNER JOIN(s) on related models. We can also take the advantage of this behaviour on nested has_many relations. Lets imagine a scenario where we have a nested has_many structure as follows:\nCountry (has_many :regions) -\u0026gt; Region (has_many :cities) -\u0026gt; City (has_many :districts) -\u0026gt; District Models and tables of the structure:\nclass Country \u0026lt; ApplicationRecord has_many :regions end | Country | | :----- | | name | class Region \u0026lt; ApplicationRecord belongs_to :country has_many :cities end | Region | | :----- | | name | | country_id | class City \u0026lt; ApplicationRecord belongs_to :region has_many :districts end | City | | :----- | | name | | region_id | class District \u0026lt; ApplicationRecord belongs_to :city end | District | | :----- | | name | | city_id | In this case, it\u0026rsquo;s quite hard to reach records after more than one level of association. It\u0026rsquo;s very possible to struggle when reaching districts of a country and you will probably end up with complicated queries. First you have to scan all regions of a specific country, then you have to scan cities of all that regions, and finally you will end up with districts of the cities, and so on. With the current structure, we are unable to run simple queries like Country.first.districts, firstly because we don\u0026rsquo;t have a country_id in our District model, and secondly there is no relationship between Country and District\nHere where has_many :through comes in to the action! Lets update our models with has_many :through without modifying the tables:\nclass Country \u0026lt; ApplicationRecord has_many :regions has_many :cities, through: :regions has_many :districts, through: :cities end class Region \u0026lt; ApplicationRecord belongs_to :country has_many :cities has_many :districts, through: :cities end class City \u0026lt; ApplicationRecord belongs_to :region has_many :districts end class District \u0026lt; ApplicationRecord belongs_to :city end And magic happens :tada: Notice that, we didn\u0026rsquo;t add any foreign_key to our models! Just plain old Rails associations. Now we are able to run any query between these models, such as:\nCountry.first.districts Region.first.districts The opposite of this relation is also quite easy. We don\u0026rsquo;t have a belongs_to :through simply because we don\u0026rsquo;t need to. You can reach the country of a district as follows:\nDistrict.first.city.region.country But how does Rails understand a relation between district and country since we don\u0026rsquo;t have foreign_keys? Simple, as I mentioned before - it conducts INNER JOIN(s). Lets look at the SQL query run:\ncountry = Country.find_by(name: \u0026#39;Turkey\u0026#39;) country.districts.to_sql =\u0026gt; \u0026#34;SELECT \u0026#34;districts\u0026#34;.* FROM \u0026#34;districts\u0026#34; INNER JOIN \u0026#34;cities\u0026#34; ON \u0026#34;districts\u0026#34;.\u0026#34;city_id\u0026#34; = \u0026#34;cities\u0026#34;.\u0026#34;id\u0026#34; INNER JOIN \u0026#34;regions\u0026#34; ON \u0026#34;cities\u0026#34;.\u0026#34;region_id\u0026#34; = \u0026#34;regions\u0026#34;.\u0026#34;id\u0026#34; WHERE \u0026#34;regions\u0026#34;.\u0026#34;country_id\u0026#34; = 209\u0026#34; Cheers.\n","permalink":"https://www.serhatdundar.com/posts/using-has-many-through-for-nested-relations-in-rails/","summary":"has_many :through is a useful association type of Rails. It\u0026rsquo;s mostly popular and often used as a join model for many-to-many relations.\nHowever, has_many :through is more than a simple join model, because it conducts INNER JOIN(s) on related models. We can also take the advantage of this behaviour on nested has_many relations. Lets imagine a scenario where we have a nested has_many structure as follows:\nCountry (has_many :regions) -\u0026gt; Region (has_many :cities) -\u0026gt; City (has_many :districts) -\u0026gt; District Models and tables of the structure:","title":"Using has_many :through for Nested Relations in Rails"},{"content":"YAML is a widely used data serialization language. In almost any software project, or during any dev-ops tasks, you can come across with YAML. For example Ruby on Rails uses YAML for fixtures, configuration files and localization. CI/CD tools such as CircleCI and Travis also use YAML for configuration. If you ever experienced a strange behaviour with YAML, you may have used one of the many reserved words of YAML. YAML reserves some words such as \u0026lsquo;yes\u0026rsquo;, \u0026lsquo;no\u0026rsquo;, \u0026lsquo;y\u0026rsquo;, \u0026lsquo;n\u0026rsquo;, \u0026lsquo;off\u0026rsquo;, \u0026lsquo;on\u0026rsquo;, etc. for boolean type. For example:\nyes: turkish: evet english: yes no: turkish: hayır english: no will be interpreted as:\ntrue: turkish: evet english: true false: turkish: hayır english: false actually YAML has a long list of reserved words for the boolean type:\ny|Y|yes|Yes|YES|n|N|no|No|NO |true|True|TRUE|false|False|FALSE |on|On|ON|off|Off|OFF In order to prevent YAML from interpreting these words as a boolean, you need to wrap them in single or double quotes as follows:\n\u0026#39;yes\u0026#39;: \u0026#39;turkish\u0026#39;: \u0026#39;evet\u0026#39; \u0026#39;german\u0026#39;: \u0026#39;ja\u0026#39; \u0026#39;english\u0026#39;: \u0026#39;yes\u0026#39; \u0026#39;no\u0026#39;: \u0026#39;turkish\u0026#39;: \u0026#39;hayır\u0026#39; \u0026#39;german\u0026#39;: \u0026#39;nein\u0026#39; \u0026#39;english\u0026#39;: \u0026#39;no\u0026#39; YAML also interprets some words to null, so wrap them inside quotes too:\n~ # (canonical) |null|Null|NULL # (English) | # (Empty) Cheers.\n","permalink":"https://www.serhatdundar.com/posts/boolean-type-words-in-yaml/","summary":"YAML is a widely used data serialization language. In almost any software project, or during any dev-ops tasks, you can come across with YAML. For example Ruby on Rails uses YAML for fixtures, configuration files and localization. CI/CD tools such as CircleCI and Travis also use YAML for configuration. If you ever experienced a strange behaviour with YAML, you may have used one of the many reserved words of YAML. YAML reserves some words such as \u0026lsquo;yes\u0026rsquo;, \u0026lsquo;no\u0026rsquo;, \u0026lsquo;y\u0026rsquo;, \u0026lsquo;n\u0026rsquo;, \u0026lsquo;off\u0026rsquo;, \u0026lsquo;on\u0026rsquo;, etc.","title":"Boolean Type Words in Yaml"},{"content":"I recently had to find similar data located in a dataset, in order to find potential duplicate records:\n\u0026quot;John Doe 123456789\u0026quot; \u0026quot;John Foe 123123123\u0026quot; After considering a couple of options, I\u0026rsquo;ve decided to continue with Elasticsearch, as it was already integrated in the project I was working on. The Ruby client of Elasticsearch provided a useful function on search results, records.each_with_hit, that I could abuse for this situation:\nfile = File.open(\u0026#34;some_file_path\u0026#34;, \u0026#34;w\u0026#34;) User.all.each do |u| r = User.search \u0026#34;*#{u.full_name}*\u0026#34; file.write(\u0026#34;Searching for: #{u.id_number}-#{u.full_name}\\n\u0026#34;) r.records.each_with_hit {|r, hit|}.map{|k, v| \u0026#34;#{k.id_number}-#{k.full_name}: #{v._score}\u0026#34;}.each do |y| unless (u.id_number == y.split(\u0026#34;-\u0026#34;)[0]) if y.split(\u0026#34;: \u0026#34;)[1].to_f \u0026gt; 1.2 file.write(\u0026#34; #{y}\\n\u0026#34;) end end end file.write(\u0026#34;***********************************\\n\\n\u0026#34;) end The 1.2 value in the script can be adjusted according to your needs. It will basically represent how close two values are. You might choose to increase it, if you want your results to cover a wider range of records, or decrease it if you are only interested in values looking very similar. Here is a sample output for the script:\nSearching for: 11156069590-JOHN DOE WHITE 33304078456-JOHN DOE BROWN: 2.3747075 22256325768-JOHN DOE BLACK: 2.2594523 33315234436-JOHN DOE BLUE: 2.229414 33378500384-JOHN DOE RED: 2.146143 11171764882-JOHN DOE GRAY: 2.146143 55577374544-JOHN DOE PURPLE: 2.0586686 Searching for: 00027522389-JOHANNA DOE 00022785363-JOHANNA DOE: 1.4142135 Cheers.\n","permalink":"https://www.serhatdundar.com/posts/calculating-similarity-between-two-data-with-ruby-and-elasticsearch/","summary":"I recently had to find similar data located in a dataset, in order to find potential duplicate records:\n\u0026quot;John Doe 123456789\u0026quot; \u0026quot;John Foe 123123123\u0026quot; After considering a couple of options, I\u0026rsquo;ve decided to continue with Elasticsearch, as it was already integrated in the project I was working on. The Ruby client of Elasticsearch provided a useful function on search results, records.each_with_hit, that I could abuse for this situation:\nfile = File.","title":"Calculating Similarity Between Two Data With Ruby and Elasticsearch"}]